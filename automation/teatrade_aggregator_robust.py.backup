#!/usr/bin/env python3
"""
TeaTrade Data Aggregator - Enhanced Version
Consolidates tea market data by location and week with smart data validation
Integrates district averages into relevant Indian auction centres
"""

import json
import os
import glob
from datetime import datetime
import re
from collections import defaultdict

def extract_week_info(filename):
    """Extract week and year information from filename"""
    # Match patterns like S37, W37, or week numbers
    week_match = re.search(r'[SW](\d+)', filename)
    year_match = re.search(r'(\d{4})', filename)
    
    if week_match:
        week = int(week_match.group(1))
    else:
        # Try to extract just numbers that might be weeks
        numbers = re.findall(r'\d+', filename)
        week = None
        for num in numbers:
            if 1 <= int(num) <= 53:  # Valid week range
                week = int(num)
                break
        if week is None:
            week = 37  # Default fallback
    
    year = int(year_match.group(1)) if year_match else 2025
    
    return week, year

def get_location_from_path(filepath):
    """Extract location from file path"""
    path_parts = filepath.split('/')
    for part in path_parts:
        if part in ['colombo', 'guwahati', 'kolkata', 'siliguri', 'cochin', 'mombasa', 'nairobi', 'district_averages']:
            return part
    return 'unknown'

def is_placeholder_data(data):
    """Detect if data contains placeholder/template content"""
    if not data or data == {}:
        return True
    
    data_str = json.dumps(data).lower()
    
    # Check for placeholder indicators
    placeholder_indicators = [
        'sample_data',
        'placeholder',
        'template',
        'n/a',
        'dynamic market commentary',
        'based on current auction data',
        'error_log'
    ]
    
    # If most values are N/A or placeholder text
    for indicator in placeholder_indicators:
        if indicator in data_str:
            return True
    
    # Check if file is too small or contains mostly empty values
    if isinstance(data, dict):
        # Count non-empty, non-placeholder values
        real_values = 0
        total_values = 0
        
        def count_real_values(obj, path=""):
            nonlocal real_values, total_values
            if isinstance(obj, dict):
                for k, v in obj.items():
                    count_real_values(v, f"{path}.{k}")
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    count_real_values(item, f"{path}[{i}]")
            else:
                total_values += 1
                if obj and str(obj).strip() and str(obj).lower() not in ['n/a', 'null', 'none', '']:
                    real_values += 1
        
        count_real_values(data)
        
        # If less than 30% real data, consider it placeholder
        if total_values > 0 and (real_values / total_values) < 0.3:
            return True
    
    return False

def extract_market_data(data, source_file):
    """Extract and standardize market data from various file formats"""
    extracted = {
        'source_file': source_file,
        'data_type': 'market_report',
        'auction_data': {},
        'price_data': {},
        'volume_data': {},
        'quality_data': {},
        'raw_data': data
    }
    
    # Handle different data structures
    if isinstance(data, dict):
        # Look for common market data fields
        for key, value in data.items():
            key_lower = key.lower()
            
            # Price related data
            if any(term in key_lower for term in ['price', 'rate', 'avg', 'average']):
                extracted['price_data'][key] = value
            
            # Volume related data
            elif any(term in key_lower for term in ['volume', 'quantity', 'kg', 'tons', 'lots']):
                extracted['volume_data'][key] = value
            
            # Quality related data
            elif any(term in key_lower for term in ['grade', 'quality', 'type', 'category']):
                extracted['quality_data'][key] = value
            
            # General auction data
            else:
                extracted['auction_data'][key] = value
    
    return extracted

def is_district_averages_file(filepath):
    """Check if file is from district_averages"""
    return 'district_averages' in filepath

def get_target_location_for_district_avg(filename):
    """Determine which Indian auction centre should receive district average data"""
    filename_lower = filename.lower()
    if 'kolkata' in filename_lower:
        return 'kolkata'
    elif 'guwahati' in filename_lower:
        return 'guwahati'
    elif 'siliguri' in filename_lower:
        return 'siliguri'
    else:
        return 'guwahati'  # Default to Guwahati as major Indian centre

def aggregate_by_location_week():
    """Main aggregation function"""
    print("Starting TeaTrade data aggregation with enhanced validation...")
    
    source_dir = "source_reports"
    output_dir = "Data/Consolidated"
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Track all available reports for the library
    all_reports = []
    
    # Group files by location and week
    location_week_data = defaultdict(lambda: defaultdict(list))
    district_avg_data = defaultdict(lambda: defaultdict(list))
    
    # Find all JSON files (excluding manifests)
    json_files = []
    for root, dirs, files in os.walk(source_dir):
        for file in files:
            if file.endswith('.json') and 'manifest' not in file.lower():
                json_files.append(os.path.join(root, file))
    
    print(f"Found {len(json_files)} JSON files to process")
    
    # Process each file
    processed_count = 0
    skipped_count = 0
    
    for filepath in json_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Check if data is placeholder/template
            if is_placeholder_data(data):
                print(f"Skipping placeholder data: {os.path.basename(filepath)}")
                skipped_count += 1
                continue
            
            # Extract location, week, and year
            location = get_location_from_path(filepath)
            week, year = extract_week_info(os.path.basename(filepath))
            
            # Extract structured market data
            market_data = extract_market_data(data, os.path.basename(filepath))
            
            # Handle district averages specially
            if is_district_averages_file(filepath):
                target_location = get_target_location_for_district_avg(os.path.basename(filepath))
                district_avg_data[target_location][(week, year)].append({
                    'source_file': os.path.basename(filepath),
                    'data_type': 'district_averages',
                    'data': market_data
                })
                print(f"District average data from {os.path.basename(filepath)} will be added to {target_location}")
            else:
                # Regular location data
                location_week_data[location][(week, year)].append(market_data)
            
            print(f"Processed: {os.path.basename(filepath)} -> {location} Week {week} {year}")
            processed_count += 1
            
        except Exception as e:
            print(f"Error processing {filepath}: {e}")
            skipped_count += 1
            continue
    
    print(f"\nProcessed {processed_count} files, skipped {skipped_count} placeholder/invalid files")
    
    # Generate consolidated files only for locations with real data
    consolidated_count = 0
    
    for location in location_week_data:
        for (week, year) in location_week_data[location]:
            # Base data for this location-week
            consolidated_data = {
                'metadata': {
                    'auction_centre': location.title(),
                    'week_number': week,
                    'year': year,
                    'generation_time': datetime.now().isoformat(),
                    'source_files': [],
                    'data_sources_count': len(location_week_data[location][(week, year)]),
                    'has_district_averages': False
                },
                'market_data': [],
                'district_averages': None,
                'summary_statistics': {
                    'total_sources': 0,
                    'data_quality': 'verified'
                }
            }
            
            # Add regular market data
            for market_data in location_week_data[location][(week, year)]:
                consolidated_data['metadata']['source_files'].append(market_data['source_file'])
                consolidated_data['market_data'].append(market_data)
                consolidated_data['summary_statistics']['total_sources'] += 1
            
            # Add district averages if available for this location-week
            if location in district_avg_data and (week, year) in district_avg_data[location]:
                district_data = []
                for district_file in district_avg_data[location][(week, year)]:
                    consolidated_data['metadata']['source_files'].append(district_file['source_file'])
                    district_data.append(district_file['data'])
                
                consolidated_data['district_averages'] = district_data
                consolidated_data['metadata']['has_district_averages'] = True
                print(f"Added district averages to {location} Week {week} {year}")
            
            # Generate filename and save
            output_filename = f"{location.title()}_S{week:02d}_{year}_consolidated.json"
            output_path = os.path.join(output_dir, output_filename)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(consolidated_data, f, indent=2, ensure_ascii=False)
            
            print(f"Created: {output_filename}")
            consolidated_count += 1
            
            # Add to reports library
            all_reports.append({
                'year': year,
                'week_number': week,
                'auction_centre': location.title(),
                'source': 'TeaTrade Consolidated',
                'title': f"{location.title()} Market Report",
                'description': f"Consolidated market data for {location.title()} - Week {week}, {year}",
                'report_link': f"report-viewer.html?dataset={location.title()}_S{week:02d}_{year}_consolidated",
                'has_district_averages': location in district_avg_data and (week, year) in district_avg_data[location],
                'data_sources': len(location_week_data[location][(week, year)]),
                'data_quality': 'verified'
            })
    
    # Sort reports by year, week, then location
    all_reports.sort(key=lambda x: (x['year'], x['week_number'], x['auction_centre']))
    
    # Generate market reports library
    library_path = "market-reports-library.json"
    with open(library_path, 'w', encoding='utf-8') as f:
        json.dump(all_reports, f, indent=2, ensure_ascii=False)
    
    print(f"\nAggregation complete!")
    print(f"Generated {consolidated_count} consolidated reports from {processed_count} valid source files")
    print(f"Created {len(all_reports)} entries in reports library")
    print(f"Reports library saved to: {library_path}")
    
    # Show summary
    location_counts = defaultdict(int)
    district_enhanced_count = 0
    
    for report in all_reports:
        location_counts[report['auction_centre']] += 1
        if report.get('has_district_averages'):
            district_enhanced_count += 1
    
    print("\nReports by auction centre:")
    for location, count in sorted(location_counts.items()):
        print(f"  {location}: {count} reports")
    
    if district_enhanced_count > 0:
        print(f"\n{district_enhanced_count} reports enhanced with district average data")
    
    print(f"\nSkipped {skipped_count} files (placeholder/template data)")

if __name__ == "__main__":
    aggregate_by_location_week()
