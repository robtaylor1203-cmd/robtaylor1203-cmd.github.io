name: Complete Tea Trade Automation

on:
  # Daily automation at 6 AM UTC
  schedule:
    - cron: '0 6 * * *'
  
  # Manual trigger with options
  workflow_dispatch:
    inputs:
      parallel_mode:
        description: 'Run scrapers in parallel'
        required: false
        default: 'true'
        type: boolean
      
      scrapers_to_run:
        description: 'Specific scrapers (comma-separated: jthomas,ceylon,forbes,tbea,atb,news)'
        required: false
        default: 'all'
        type: string
      
      skip_post_processing:
        description: 'Skip post-processing tasks'
        required: false
        default: 'false'
        type: boolean

jobs:
  # Job 1: Complete Tea Market Data Collection
  tea-market-automation:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hours maximum
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: secure_password_123
          POSTGRES_USER: tea_admin
          POSTGRES_DB: tea_trade_data
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    env:
      PGHOST: localhost
      PGPORT: 5432
      PGUSER: tea_admin
      PGPASSWORD: secure_password_123
      PGDATABASE: tea_trade_data
    
    steps:
      - name: ğŸ›’ Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: ğŸ Setup Python environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: ğŸ“¦ Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            tesseract-ocr \
            tesseract-ocr-eng \
            libreoffice \
            pandoc \
            chromium-browser \
            postgresql-client
      
      - name: ğŸ“‹ Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r automation_requirements.txt
          playwright install chromium --with-deps
      
      - name: ğŸ—„ï¸ Initialize database
        run: |
          # Wait for PostgreSQL to be ready
          until pg_isready; do sleep 1; done
          
          # Create database schema
          psql -f create_complete_database.sql
          
          echo "âœ… Database initialized successfully"
      
      - name: ğŸ” System health check
        id: health-check
        run: |
          echo "ğŸ¥ Performing system health check..."
          
          # Check available memory
          free -h
          
          # Check disk space
          df -h
          
          # Check database connectivity
          psql -c "SELECT 'Database connected successfully';"
          
          # Check Python environment
          python --version
          python -c "import playwright, requests, psycopg2, pandas; print('âœ… All dependencies available')"
          
          echo "health_status=healthy" >> $GITHUB_OUTPUT
          echo "âœ… System health check passed"
      
      - name: ğŸš€ Run complete automation
        id: run-automation
        run: |
          echo "ğŸš€ Starting complete tea market automation..."
          
          # Determine scrapers to run
          if [ "${{ github.event.inputs.scrapers_to_run }}" = "all" ] || [ -z "${{ github.event.inputs.scrapers_to_run }}" ]; then
            SCRAPERS_ARG=""
          else
            # Convert comma-separated to space-separated
            SCRAPERS=$(echo "${{ github.event.inputs.scrapers_to_run }}" | tr ',' ' ')
            SCRAPERS_ARG="--scrapers $SCRAPERS"
          fi
          
          # Determine parallel mode
          if [ "${{ github.event.inputs.parallel_mode }}" = "false" ]; then
            PARALLEL_ARG="--sequential"
          else
            PARALLEL_ARG="--parallel"
          fi
          
          # Run master automation controller
          cd automation
          python master_controller.py $PARALLEL_ARG $SCRAPERS_ARG
          
          # Check if automation was successful
          if [ $? -eq 0 ]; then
            echo "automation_status=success" >> $GITHUB_OUTPUT
            echo "âœ… Automation completed successfully"
          else
            echo "automation_status=failed" >> $GITHUB_OUTPUT
            echo "âŒ Automation failed"
            exit 1
          fi
      
      - name: ğŸ“Š Generate market reports
        if: success() && github.event.inputs.skip_post_processing != 'true'
        run: |
          echo "ğŸ“Š Generating market reports..."
          
          # Generate weekly reports by auction centre
          psql -c "
            -- Generate weekly market reports
            INSERT INTO market_reports (
              source, title, report_type, country, report_date, 
              week_number, year, summary, key_metrics, processed
            )
            SELECT 
              'AUTOMATED_ANALYSIS',
              CONCAT('Weekly Market Report - ', auction_centre, ' - Week ', week_number, ' ', year),
              'Weekly',
              country,
              week_start_date,
              week_number,
              year,
              CONCAT('Total auctions: ', total_lots, '; Average price: ', ROUND(avg_price, 2), '; Total volume: ', total_quantity_kg, ' kg'),
              json_build_object(
                'total_lots', total_lots,
                'avg_price', avg_price,
                'min_price', min_price,
                'max_price', max_price,
                'total_quantity_kg', total_quantity_kg,
                'unique_grades', unique_grades,
                'total_gardens', total_gardens
              ),
              true
            FROM v_weekly_market_summary
            WHERE week_start_date >= CURRENT_DATE - INTERVAL '7 days'
            ON CONFLICT DO NOTHING;
          "
          
          echo "âœ… Market reports generated"
      
      - name: ğŸŒ Generate website data exports
        if: success()
        run: |
          echo "ğŸŒ Generating website data exports..."
          
          # Create data directory
          mkdir -p data/latest
          
          # Export summary data
          psql -t -c "
            SELECT json_build_object(
              'last_updated', NOW(),
              'total_auctions', (SELECT COUNT(*) FROM auction_lots WHERE scrape_timestamp >= CURRENT_DATE),
              'total_news', (SELECT COUNT(*) FROM news_articles WHERE scrape_timestamp >= CURRENT_DATE),
              'active_gardens', (SELECT COUNT(DISTINCT garden_id) FROM auction_lots WHERE scrape_timestamp >= CURRENT_DATE - INTERVAL '7 days'),
              'sources', (
                SELECT json_agg(json_build_object(
                  'source', source,
                  'lots', lots,
                  'avg_price', avg_price,
                  'last_updated', last_updated
                ))
                FROM (
                  SELECT 
                    source,
                    COUNT(*) as lots,
                    ROUND(AVG(price_per_kg), 2) as avg_price,
                    MAX(scrape_timestamp) as last_updated
                  FROM auction_lots
                  WHERE scrape_timestamp >= CURRENT_DATE - INTERVAL '7 days'
                  GROUP BY source
                ) sources
              ),
              'system_status', 'operational'
            );
          " > data/latest/summary.json
          
          # Export recent auction data for charts
          psql -t -c "
            SELECT json_agg(json_build_object(
              'auction_centre', auction_centre,
              'garden', garden,
              'grade', grade,
              'quantity_kg', quantity_kg,
              'price_per_kg', price_per_kg,
              'auction_date', auction_date,
              'source', source
            ))
            FROM (
              SELECT 
                ac.name as auction_centre,
                g.name as garden,
                al.grade,
                al.quantity_kg,
                al.price_per_kg,
                al.auction_date,
                al.source
              FROM auction_lots al
              JOIN auction_centres ac ON al.auction_centre_id = ac.id
              JOIN gardens g ON al.garden_id = g.id
              WHERE al.auction_date >= CURRENT_DATE - INTERVAL '30 days'
              ORDER BY al.auction_date DESC
              LIMIT 1000
            ) recent_auctions;
          " > data/latest/auction_data.json
          
          # Export recent news
          psql -t -c "
            SELECT json_agg(json_build_object(
              'title', title,
              'source', source,
              'url', url,
              'publish_date', publish_date,
              'summary', SUBSTRING(content, 1, 200)
            ))
            FROM news_articles
            WHERE scrape_timestamp >= CURRENT_DATE - INTERVAL '7 days'
            ORDER BY publish_date DESC
            LIMIT 50;
          " > data/latest/news_data.json
          
          # Generate chart data
          psql -t -c "
            SELECT json_build_object(
              'price_trends', (
                SELECT json_agg(json_build_object(
                  'date', date,
                  'avg_price', avg_price,
                  'total_quantity', total_quantity,
                  'auction_centre', auction_centre
                ))
                FROM (
                  SELECT 
                    DATE(al.auction_date) as date,
                    AVG(al.price_per_kg) as avg_price,
                    SUM(al.quantity_kg) as total_quantity,
                    ac.name as auction_centre
                  FROM auction_lots al
                  JOIN auction_centres ac ON al.auction_centre_id = ac.id
                  WHERE al.auction_date >= CURRENT_DATE - INTERVAL '30 days'
                  GROUP BY DATE(al.auction_date), ac.name
                  ORDER BY date DESC
                ) trends
              ),
              'top_gardens', (
                SELECT json_agg(json_build_object(
                  'garden', garden,
                  'total_quantity', total_quantity,
                  'avg_price', avg_price,
                  'lots_count', lots_count
                ))
                FROM (
                  SELECT 
                    g.name as garden,
                    SUM(al.quantity_kg) as total_quantity,
                    ROUND(AVG(al.price_per_kg), 2) as avg_price,
                    COUNT(*) as lots_count
                  FROM auction_lots al
                  JOIN gardens g ON al.garden_id = g.id
                  WHERE al.auction_date >= CURRENT_DATE - INTERVAL '30 days'
                  GROUP BY g.name
                  ORDER BY total_quantity DESC
                  LIMIT 20
                ) gardens
              )
            );
          " > data/latest/chart_data.json
          
          echo "âœ… Website data exports generated"
      
      - name: ğŸ” Validate data quality
        if: success()
        run: |
          echo "ğŸ” Validating data quality..."
          
          # Check if data files are valid JSON
          for file in data/latest/*.json; do
            if python -m json.tool "$file" > /dev/null 2>&1; then
              echo "âœ… $file is valid JSON"
            else
              echo "âŒ $file is invalid JSON"
              exit 1
            fi
          done
          
          # Check data completeness
          auction_count=$(psql -t -c "SELECT COUNT(*) FROM auction_lots WHERE scrape_timestamp >= CURRENT_DATE;")
          news_count=$(psql -t -c "SELECT COUNT(*) FROM news_articles WHERE scrape_timestamp >= CURRENT_DATE;")
          
          echo "ğŸ“Š Data collected today:"
          echo "  - Auction lots: $auction_count"
          echo "  - News articles: $news_count"
          
          # Set minimum thresholds
          if [ "$auction_count" -lt 10 ]; then
            echo "âš ï¸ Warning: Low auction data count ($auction_count)"
          fi
          
          echo "âœ… Data quality validation completed"
      
      - name: ğŸ¨ Update website with live data
        if: success()
        run: |
          echo "ğŸ¨ Updating website with live data..."
          
          # Update your beautiful index.html with timestamp
          if [ -f "index.html" ]; then
            # Remove old timestamp
            sed -i '/<!-- Last automated update:/d' index.html
            
            # Add new timestamp
            sed -i '/<\/body>/i\    <!-- Last automated update: '"$(date -u '+%Y-%m-%d %H:%M:%S UTC')"' -->' index.html
            
            echo "âœ… Website timestamp updated"
          fi
          
          # Ensure your enhanced JavaScript can access the data
          if [ -f "assets/js/main.js" ]; then
            echo "âœ… Enhanced JavaScript ready for live data"
          fi
      
      - name: ğŸ“ Generate execution report
        if: always()
        run: |
          echo "ğŸ“ Generating execution report..."
          
          # Generate comprehensive execution report
          cat > execution_report.md << EOF
          # Tea Trade Automation Execution Report
          
          **Execution Time**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          **Workflow**: Complete Tea Market Data Collection
          **Status**: ${{ steps.run-automation.outputs.automation_status }}
          
          ## System Health
          - **Health Check**: ${{ steps.health-check.outputs.health_status }}
          - **Database**: Connected successfully
          - **Dependencies**: All available
          
          ## Data Collection Summary
          EOF
          
          # Add auction data summary
          auction_summary=$(psql -t -c "
            SELECT 
              source || ': ' || COUNT(*) || ' lots'
            FROM auction_lots 
            WHERE scrape_timestamp >= CURRENT_DATE
            GROUP BY source;
          ")
          
          echo "### Auction Data" >> execution_report.md
          echo "$auction_summary" | while read line; do
            echo "- $line" >> execution_report.md
          done
          
          # Add news summary
          news_summary=$(psql -t -c "
            SELECT COUNT(*) || ' news articles collected'
            FROM news_articles 
            WHERE scrape_timestamp >= CURRENT_DATE;
          ")
          
          echo -e "\n### News Data" >> execution_report.md
          echo "- $news_summary" >> execution_report.md
          
          echo -e "\n## Next Execution" >> execution_report.md
          echo "**Scheduled**: $(date -u -d '+1 day' '+%Y-%m-%d 06:00 UTC')" >> execution_report.md
          
          echo "âœ… Execution report generated"
      
      - name: ğŸ’¾ Commit and push updates
        if: success()
        run: |
          echo "ğŸ’¾ Committing data updates..."
          
          # Configure Git
          git config --local user.email "action@github.com"
          git config --local user.name "Tea Trade Automation Bot"
          
          # Add all data files
          git add data/latest/
          
          # Add website updates
          git add index.html || true
          git add execution_report.md
          
          # Check if there are changes to commit
          if ! git diff --staged --quiet; then
            # Generate commit message with statistics
            auction_count=$(psql -t -c "SELECT COUNT(*) FROM auction_lots WHERE scrape_timestamp >= CURRENT_DATE;" | tr -d ' ')
            news_count=$(psql -t -c "SELECT COUNT(*) FROM news_articles WHERE scrape_timestamp >= CURRENT_DATE;" | tr -d ' ')
            
            commit_message="ğŸ¤– Automated data update $(date -u '+%Y-%m-%d %H:%M UTC')

ğŸ¯ Collection Summary:
- ğŸ“Š Auction lots: $auction_count
- ğŸ“° News articles: $news_count
- ğŸ• Next update: $(date -u -d '+1 day' '+%Y-%m-%d 06:00 UTC')

âœ¨ Your beautiful TeaTrade design enhanced with fresh market data
ğŸš€ System Status: Operational
ğŸ”„ Automation: ${{ steps.run-automation.outputs.automation_status }}"

            git commit -m "$commit_message"
            git push
            
            echo "âœ… Updates committed and pushed successfully"
          else
            echo "ğŸ“ No changes to commit"
          fi

  # Job 2: Deploy Website (if main branch)
  deploy-website:
    needs: tea-market-automation
    if: github.ref == 'refs/heads/main' && success()
    runs-on: ubuntu-latest
    
    permissions:
      contents: read
      pages: write
      id-token: write
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
      - name: ğŸ›’ Checkout updated repository
        uses: actions/checkout@v4
        with:
          ref: main
      
      - name: ğŸ”§ Setup GitHub Pages
        uses: actions/configure-pages@v3
      
      - name: ğŸ“¦ Upload Pages artifact
        uses: actions/upload-pages-artifact@v2
        with:
          path: '.'
      
      - name: ğŸš€ Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v2
      
      - name: âœ… Deployment complete
        run: |
          echo "ğŸ‰ Website deployed successfully!"
          echo "ğŸŒ URL: ${{ steps.deployment.outputs.page_url }}"
          echo "ğŸ• Deployed at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
